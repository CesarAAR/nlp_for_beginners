# ğŸ§  IntroducciÃ³n al Procesamiento de Lenguaje Natural (NLP)

Este cuaderno (`.ipynb`) contiene una introducciÃ³n prÃ¡ctica a los conceptos fundamentales del **Procesamiento de Lenguaje Natural (NLP)**, un Ã¡rea de la inteligencia artificial que busca permitir que las computadoras comprendan, interpreten y generen lenguaje humano.

El objetivo de este notebook es servir como material de referencia y prÃ¡ctica para comprender los principios bÃ¡sicos detrÃ¡s de las tÃ©cnicas mÃ¡s comunes en NLP, desde la **tokenizaciÃ³n** hasta la **similitud de vectores**.

---

## ğŸ“˜ Contenido del cuaderno

### 1ï¸âƒ£ TokenizaciÃ³n
La **tokenizaciÃ³n** es el primer paso en la mayorÃ­a de los procesos de NLP.  
Consiste en dividir un texto en unidades mÃ¡s pequeÃ±as llamadas *tokens* (palabras, frases o incluso caracteres).  
Esto permite que el texto sea analizado y procesado de forma estructurada.

ğŸ“ *Ejemplo:* dividir `"El lenguaje natural es fascinante"` en `["El", "lenguaje", "natural", "es", "fascinante"]`.

---

### 2ï¸âƒ£ Stop Words
Las **stop words** son palabras comunes (como *el, la, de, en, y*) que generalmente se eliminan porque no aportan valor semÃ¡ntico relevante al anÃ¡lisis.  
El objetivo es reducir el ruido en el procesamiento de texto.

ğŸ“ *Ejemplo:* eliminar â€œelâ€ o â€œdeâ€ de un texto antes del anÃ¡lisis.

---

### 3ï¸âƒ£ Stemming y LemmatizaciÃ³n
Ambas tÃ©cnicas buscan reducir las palabras a su forma base:

- **Stemming:** recorta palabras eliminando sufijos y prefijos, sin considerar el contexto gramatical.  
  ğŸ“ *Ejemplo:* â€œcorriendoâ€, â€œcorriÃ³â€ â†’ â€œcorrâ€.

- **LematizaciÃ³n:** transforma las palabras a su forma raÃ­z (*lema*), considerando su estructura y significado.  
  ğŸ“ *Ejemplo:* â€œcorriendoâ€, â€œcorriÃ³â€ â†’ â€œcorrerâ€.

Estas tÃ©cnicas ayudan a normalizar el texto y mejorar la precisiÃ³n de los modelos.

---

### 4ï¸âƒ£ IntroducciÃ³n a la Similitud de Vectores
En NLP, los textos se representan como **vectores numÃ©ricos**, lo que permite medir su similitud.  
Se utilizan mÃ©tricas como el **cosine similarity** para determinar quÃ© tan parecidos son dos documentos o frases.

ğŸ“ *Ejemplo:* comparar la similitud entre  
> â€œEl perro corre rÃ¡pidoâ€  
> â€œEl gato corre velozâ€

---

### 5ï¸âƒ£ IntroducciÃ³n al MÃ©todo TF-IDF
El **TF-IDF (Term Frequency â€“ Inverse Document Frequency)** es una tÃ©cnica estadÃ­stica que evalÃºa la importancia de una palabra dentro de un documento en relaciÃ³n con una colecciÃ³n de documentos.

- **TF (Frecuencia de tÃ©rmino):** mide cuÃ¡ntas veces aparece una palabra en un documento.  
- **IDF (Frecuencia inversa de documentos):** mide la relevancia global de la palabra en el conjunto de textos.

ğŸ“ *Ejemplo:* Palabras como *â€œdeâ€* o *â€œelâ€* tienen bajo peso TF-IDF, mientras que tÃ©rminos mÃ¡s especÃ­ficos como *â€œmachine learningâ€* tienen alto peso.

---

### 6ï¸âƒ£ TF-IDF PrÃ¡ctico y Word Embeddings Neuronales  
Esta nueva secciÃ³n amplÃ­a las notas anteriores con:

#### ğŸ§© Parte PrÃ¡ctica de TF-IDF  
AplicaciÃ³n prÃ¡ctica del mÃ©todo TF-IDF con un conjunto de textos para comparar su rendimiento frente a la representaciÃ³n basada en frecuencia de palabras.  
Se analizan los resultados y se visualizan las puntuaciones TF-IDF mÃ¡s relevantes.

#### ğŸ§  IntroducciÃ³n a Word Embeddings Neuronales  
IntroducciÃ³n teÃ³rica a los **word embeddings neuronales**, un enfoque mÃ¡s avanzado que permite capturar relaciones semÃ¡nticas entre palabras mediante representaciones vectoriales densas.  
Se explican conceptos como:
- Espacios vectoriales semÃ¡nticos  
- AnalogÃ­as de palabras (por ejemplo, â€œRey - Hombre + Mujer = Reinaâ€)  
- Modelos populares como **Word2Vec** y **GloVe**

---
## ğŸ§© TecnologÃ­as y librerÃ­as utilizadas
- **Python 3**
- **NLTK**
- **spaCy**
- **scikit-learn**
- **NumPy / Pandas**
- **Word2Vec**
- **pymupdf**

---

## ğŸ¯ Objetivo del proyecto
Este notebook tiene fines educativos y de autoaprendizaje.  
Permite comprender los conceptos bÃ¡sicos de procesamiento de texto y sirve como base para avanzar hacia temas mÃ¡s complejos, como:

- Modelos de lenguaje (Word2Vec, GloVe, BERT)
- ClasificaciÃ³n de texto
- AnÃ¡lisis de sentimiento
- ExtracciÃ³n de entidades
- Chatbots y generaciÃ³n de texto

---

## ğŸ“‚ Estructura sugerida del repositorio
ğŸ“¦ NLP_TESTING

â”£ ğŸ“œ README.md

â”£ ğŸ““ nlp_introduction.ipynb
<<<<<<< HEAD

=======
â”£ ğŸ““ nlp_introduction_part2.ipynb
â”£ texto_text.txt
>>>>>>> 1fa53bc (Nlp part 2)
â”— df_total.csv (ejemplo)

---

## ğŸ§  PrÃ³ximos pasos
Se realizaran mÃ¡s publicaciones tocando temas desde intermedios hasta avanzados.

---

âœï¸ **Autor:** *CÃ©sar Ãlvarez*  
ğŸ“… **AÃ±o:** 2025  
ğŸ“˜ **Fuente de aprendizaje:** Apuntes personales sobre [Curso Completo de Procesamiento de Lenguaje Natural (NLP) con Python](https://youtu.be/9x1QtYNLJRY?si=lSJt37JcyXwNNBKk) del canal [CÃ³digo Espinoza - IA y Machine Learning](https://www.youtube.com/@codigoespinozaIA)
