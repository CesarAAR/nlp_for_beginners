{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5ac9b5",
   "metadata": {},
   "source": [
    "## Tokenización\n",
    "Convertir textos a tokens.\n",
    "\n",
    "Son caracteres o palabras unicas de los textos.\n",
    "\n",
    "Digamos que tenemos la siguiente frase:\n",
    "\n",
    "\"El perro está dormido en el sillon\" \n",
    "\n",
    "Esa frase es igual a 6 Tokens (7 tokens si el objetivo es hacer sensible el modelo a mayusculas y minusculas).\n",
    "\n",
    "Existen varias formas para tokenizar: \n",
    "- Por palabra (\"Hola, fernando\" = \"Hola,\",\"fernando\")\n",
    "- Por caracter (\"automóvil\" = \"a\",\"u\",\"t\",\"o\",...,\"l\")\n",
    "- Por Sub palabras (\"automóvil\" = \"auto\", \"móvil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dcc9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola, ¿cómo estás?\n",
      "['hola,', '¿cómo', 'estás?']\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo de tokenización\n",
    "texto = \"Hola, ¿Cómo estás?\"\n",
    "texto = texto.lower() #tomaremos como base un modelo que no es sensible a mayusculas y minuscilas\n",
    "print(texto)\n",
    "tokens = texto.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89700cd",
   "metadata": {},
   "source": [
    "Explicación de la respuesta: Se puede observar en la respuesta que las palabras vienen con caracteres especiales (\",\", \"¿\", \"?\"). Esto es por la naturaleza del split donde \"tokenizará\" un texto hasta encontrar el siguiente espacio. En este punto se puede limpiar el texto para quitar dichos caracteres especiales o tambien tokenizar dichos caracteres, eso dependerá del objetivo de lo que se busca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4c5a1",
   "metadata": {},
   "source": [
    "## STOPWORDS\n",
    "\n",
    "KeepCoding define las stopwords como \"conjunto de palabras comunes en un idioma que se filtran o excluyen del análisis de texto durante el procesamiento de lenguaje natural\".\n",
    "\n",
    "Unos ejemplos en el lenguaje español serian:\n",
    "- y\n",
    "- la\n",
    "- los\n",
    "- en\n",
    "- para\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc99f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se descargan la lista de stopwords de nltk\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "908ab01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Anclamos que queremos los stopwords del idioma español\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a0545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\cachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora tokenizaremos nuestro texto\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa03c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', ',', '¿cómo', 'estás', '?']\n"
     ]
    }
   ],
   "source": [
    "texto = \"Hola, ¿Cómo estás?\"\n",
    "texto = texto.lower()\n",
    "tokens = word_tokenize(texto,language=\"spanish\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e70495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', ',', '¿cómo', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_filtrado = [word for word in tokens if not word in stop_words]\n",
    "texto_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa2e2d",
   "metadata": {},
   "source": [
    "Explicación de salida: ¿Por qué la \", ¿ ?\" se mantuvieron y el \"estás\" se borró?\n",
    "\n",
    "En nuestras stopwords no se encuentran dichos caracteres especiales pero al parecer el \"estás\" sí se encuentra en las stopwords por lo que por esa misma razón nos dio ese resultado pero que tal si probamos con la siguiente frase:\n",
    "\n",
    "\"El fruto se cayó del arbol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8603fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['el', 'fruto', 'se', 'cayó', 'del', 'arbol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fruto', 'cayó', 'arbol']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"El fruto se cayó del arbol\"\n",
    "texto = texto.lower()\n",
    "tokens = word_tokenize(texto,language=\"spanish\")\n",
    "print(tokens)\n",
    "\n",
    "texto_filtrado = [word for word in tokens if not word in stop_words]\n",
    "texto_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb520c8",
   "metadata": {},
   "source": [
    "Se puede observar que solo se dejó aquellas palabras que no son stopwords (fruto, cayó, arbol).\n",
    "\n",
    "¿Cómo sería agregar más palabras a mi stopwords?\n",
    "\n",
    "Existen 2 formas:\n",
    "- Modificar el archivo de los stopwords (En local se encuentra en: C:\\Users\\[user_name]\\AppData\\Roaming\\nltk_data\\corpora\\stopwords, buscar el lenguaje que deseas, abrir dicho documento con bloc de notas y agregar las palabras que desees)\n",
    "\n",
    "- crear otra variable con el mismo dataset de stopwords y agregar lo que deseas, como el ejemplo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9156db8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " '?',\n",
       " 'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " '¿',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_words = stop_words\n",
    "s_words.update(\",\",\"¿\",\"?\") #add si solo agregaras un elemento; update si agregarás más de un elemento.\n",
    "s_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77732ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', ',', '¿', 'cómo', 'estás', '?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hola', 'cómo']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"Hola, ¿ Cómo estás?\"\n",
    "texto = texto.lower()\n",
    "tokens = word_tokenize(texto,language=\"spanish\")\n",
    "print(tokens)\n",
    "\n",
    "texto_filtrado = [word for word in tokens if not word in s_words]\n",
    "texto_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049bd57",
   "metadata": {},
   "source": [
    "Observación: Al parecer cuando se usa el \"¿\" el tokenizador lo toma conjunto a la palabra siguiente por lo que puede llegar a generar ciertos problemas, se sugieren más pruebas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23e9b6",
   "metadata": {},
   "source": [
    "## Stemming y Lemmatization\n",
    "\n",
    "### Algunos problemas de la tokenización básica y recuentos son los siguientes:\n",
    "- **Las palabras similares se tratan como entidades separadas**. \n",
    "\n",
    "Ejemplo: Caminar, Caminando, Camina; no importa si las palabras esten estrechamente relacionadas, estas se tomarán como palabras independientes de las otras.\n",
    "- **El alto dimensional de los vectores resultantes**. \n",
    "\n",
    "Pueden existir palabras con significados similares pero el tokenizador los tomará como palabras diferentes (el mismo casó del punto anterior). El mejor caso seria que si se encuentran las palabras \"caminar, caminando, camina, caminó\" se tome bajo el mismo verbo que seria \"caminar\", esto para que no tenga que contabilizar 4 veces una palabra que se podria resumir a 1 sola.\n",
    "- **Aplicaciones practicas y desventajas de la tokenización basica**. \n",
    "\n",
    "Ejemplo: Digamos que estamos realizando un motor de busqueda y coloco \"Persona corriendo\", lo mejor sería que me coloque resultados con relación al verbo \"correr\" y no solo mostrar resultados de, literal, \"persona CORRIENDO\" lo cual disminuiría la cantidad de resultados haciendo que el usuario tenga que buscar basado en el tiempo del verbo/accion (pasado, presente y futuro) para obtener el resultado que el busca en vez de mostrar el resultado de todos los tiempo.\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "El Stemming es una técnica más simple que simplemente elimina los sufijos de las palabras.\n",
    "\n",
    "Ejemplo:\n",
    "- Spanish: ando, endo, ido, etc\n",
    "- English: ing, ed, s, en\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "La Lemmatization es una técnica más sofisticada que utiliza las reglas del lenguaje para obtener la base o raiz de una palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab84b7",
   "metadata": {},
   "source": [
    "Ejemplo Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6744752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Descargar las palabras que nos serviran para realizar el stemming\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20740447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camin\n",
      "camin\n",
      "camin\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo en español\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "#creamos el stemmer en español\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "print(stemmer.stem('caminando'))\n",
    "print(stemmer.stem('caminar'))\n",
    "print(stemmer.stem('caminó'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b3aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n",
      "run\n",
      "run\n",
      "ran\n",
      "swim\n",
      "swim\n",
      "swam\n",
      "swum\n"
     ]
    }
   ],
   "source": [
    "#ejemplo en ingles\n",
    "#creamos el stemmer en ingles\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print(stemmer.stem('walking'))\n",
    "print(stemmer.stem('walk'))\n",
    "print(stemmer.stem('walked'))\n",
    "\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('run'))\n",
    "print(stemmer.stem('ran'))\n",
    "\n",
    "print(stemmer.stem('swimming'))\n",
    "print(stemmer.stem('swim'))\n",
    "print(stemmer.stem('swam'))\n",
    "print(stemmer.stem('swum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc234e",
   "metadata": {},
   "source": [
    "Observaciones: Apesar de que en ingles se obtuvo buenos resultados, este llega a fallar con verbos irregulares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a28cb",
   "metadata": {},
   "source": [
    "Ejmeplo Lemmatization\n",
    "\n",
    "#instalaciones necesarias si ejecutas de forma local\n",
    "```\n",
    "pip install spacy\n",
    "!python -m spacy download es_core_news_sm -q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08afe5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caminar  ->  caminar\n",
      "caminando  ->  caminar\n",
      "caminó  ->  caminar\n",
      "caminé  ->  caminé\n",
      "caminaba  ->  caminar\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#cargar el modelo en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "#crear un documento\n",
    "doc = nlp(\"caminar caminando caminó caminé caminaba\")\n",
    "\n",
    "#imprimir el texto y el lemma de cada token\n",
    "for token in doc:\n",
    "    print(token.text, \" -> \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1514c2e",
   "metadata": {},
   "source": [
    "Observaciones: El resultado de dicha prueba demuestra que si se puede lograr el objetivo de disminuir las palabras a su \"verbo\" base, aunque existen excepciones a la regla como lo es la palabra \"caminé\", se sugieren más pruebas con dicha libreria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3759c",
   "metadata": {},
   "source": [
    "**Particularidades del Lemmatization y recomendaciones para su uso**\n",
    "- La lemmatization puede ser más efectiva que el stemming, pero tambien es más costosa computacionalmente.\n",
    "- El uso de lemmatization puede requerir el etiquetado previo. Esto se pudo observar con el caso de \"caminé\" donde quizás dicha palabra no está dentro del conjunto de palabras para el verbo \"caminar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1186d2",
   "metadata": {},
   "source": [
    "# ÁREA PRACTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"kevinmorgado/spanish-news-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2452877",
   "metadata": {},
   "source": [
    "## Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add57d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86661b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>news</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3201905</td>\n",
       "      <td>Durante el foro La banca articulador empresari...</td>\n",
       "      <td>Otra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3210288</td>\n",
       "      <td>El regulador de valores de China dijo el domin...</td>\n",
       "      <td>Regulaciones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3240676</td>\n",
       "      <td>En una industria históricamente masculina como...</td>\n",
       "      <td>Alianzas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3342889</td>\n",
       "      <td>Con el dato de marzo el IPC interanual encaden...</td>\n",
       "      <td>Macroeconomia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3427208</td>\n",
       "      <td>Ayer en Cartagena se dio inicio a la versión n...</td>\n",
       "      <td>Otra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>https://www.bbva.com/es/como-lograr-que-los-in...</td>\n",
       "      <td>En la vida de toda empresa emergente llega un ...</td>\n",
       "      <td>Innovacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>https://www.bbva.com/es/podcast-como-nos-afect...</td>\n",
       "      <td>La espiral alcista de los precios continúa y g...</td>\n",
       "      <td>Macroeconomia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3253735</td>\n",
       "      <td>Las grandes derrotas nacionales son experienci...</td>\n",
       "      <td>Alianzas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>https://www.bbva.com/es/bbva-y-barcelona-healt...</td>\n",
       "      <td>BBVA ha alcanzado un acuerdo de colaboración c...</td>\n",
       "      <td>Innovacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3263980</td>\n",
       "      <td>Casi entrando a la parte final de noviembre la...</td>\n",
       "      <td>Alianzas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1217 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0      https://www.larepublica.co/redirect/post/3201905   \n",
       "1      https://www.larepublica.co/redirect/post/3210288   \n",
       "2      https://www.larepublica.co/redirect/post/3240676   \n",
       "3      https://www.larepublica.co/redirect/post/3342889   \n",
       "4      https://www.larepublica.co/redirect/post/3427208   \n",
       "...                                                 ...   \n",
       "1212  https://www.bbva.com/es/como-lograr-que-los-in...   \n",
       "1213  https://www.bbva.com/es/podcast-como-nos-afect...   \n",
       "1214   https://www.larepublica.co/redirect/post/3253735   \n",
       "1215  https://www.bbva.com/es/bbva-y-barcelona-healt...   \n",
       "1216   https://www.larepublica.co/redirect/post/3263980   \n",
       "\n",
       "                                                   news           Type  \n",
       "0     Durante el foro La banca articulador empresari...           Otra  \n",
       "1     El regulador de valores de China dijo el domin...   Regulaciones  \n",
       "2     En una industria históricamente masculina como...       Alianzas  \n",
       "3     Con el dato de marzo el IPC interanual encaden...  Macroeconomia  \n",
       "4     Ayer en Cartagena se dio inicio a la versión n...           Otra  \n",
       "...                                                 ...            ...  \n",
       "1212  En la vida de toda empresa emergente llega un ...     Innovacion  \n",
       "1213  La espiral alcista de los precios continúa y g...  Macroeconomia  \n",
       "1214  Las grandes derrotas nacionales son experienci...       Alianzas  \n",
       "1215  BBVA ha alcanzado un acuerdo de colaboración c...     Innovacion  \n",
       "1216  Casi entrando a la parte final de noviembre la...       Alianzas  \n",
       "\n",
       "[1217 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df_total.csv', encoding='UTF-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1e5940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separamos los datos en variables de entrada y etiqueta\n",
    "X = df['news']\n",
    "y = df['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cb42114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type\n",
      "Macroeconomia     340\n",
      "Alianzas          247\n",
      "Innovacion        195\n",
      "Regulaciones      142\n",
      "Sostenibilidad    137\n",
      "Otra              130\n",
      "Reputacion         26\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "892fb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23f16af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65d8158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8073770491803278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d73d19b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 235186 stored elements and shape (973, 26760)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vemos dimensionalidad\n",
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828fffd",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4409e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8bf5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29fbb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stems = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1f4ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['news_stemmer'] = df['news'].apply(tokenize_and_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "943aca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       durant el for la banc articul empresarial par ...\n",
       "1       el regul de valor de chin dij el doming que bu...\n",
       "2       en una industri histor masculin com lo es la a...\n",
       "3       con el dat de marz el ipc interanual encaden s...\n",
       "4       ayer en cartagen se dio inici a la version num...\n",
       "                              ...                        \n",
       "1212    en la vid de tod empres emergent lleg un momen...\n",
       "1213    la espiral alcist de los preci continu y gener...\n",
       "1214    las grand derrot nacional son experient trauma...\n",
       "1215    bbva ha alcanz un acuerd de colabor con barcel...\n",
       "1216    casi entrand a la part final de noviembr la ep...\n",
       "Name: news_stemmer, Length: 1217, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['news_stemmer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fe5c24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'con el dat de marz el ipc interanual encaden su decimoquint tas posit consecut la inflacion public por el ine se ha manten igual respect al avanc del de marz y se situ punt por encim del dat de febrer que ascend al esos punt de diferent la mayor part la coloc el grup de la viviend punt por la sub de la electr y el del transport punt por el alza de los carbur tambien impuls el ipc de marz el aument de los preci de la restaur y los servici de aloj y al encarec generaliz de los aliment especial del pesc y el marisc de la carn de las legumbr y hortaliz y de la lech el ques y los ten en cuent la rebaj del impuest especial sobr la electr y las variacion sobr otros impuest el ipc interanual alcanz en marz nuev decim mas que la tas general del asi lo reflej el ipc a impuest constant que el ine tambien public en el marc de esta inflacion subyacent sin aliment no elabor ni product energet aument en marz cuatr decim hast su valor mas alto desd septiembr de de este mod la subyacent se situ mas de seis punt por debaj de la tas del ipc el ultim año la calefaccion el alumbr y la distribu de agu se han encarec los aceit y gras han elev sus preci un y el transport personal es un mas car por el mayor cost de los carbur tambien registr alzas de dos digit los huev y la lech un mas car que hac un año y la carn de ovin y el pesc fresc y congel con repunt del en ambos estan los preci por comunidadescastillal manch se situ a la cabez con una tas de inflacion del segu de castill y leon aragon la rioj galici extremadur cantabri y comun valencian el otro las comun dond se registr las menor sub fueron canari madr balear asturi pais vasc y cataluñ ipc disp su tas mensual al tas mensual el ipc registr en marz un increment del respect a febrer su mayor alza mensual en cualqui mes desd cuand se camb la metodolog de esta estadist par recog mejor la evolu del merc echand la vist mas atras tom seri anterior el repunt mensual de marz es el mas elev desd agost de el terc mes de el indic de preci de consum armoniz ipca situ su tas interanual en mas de dos punt por encim de la de febrer por su part el indic adelant del ipca avanz un en tas mensual'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['news_stemmer'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7b7e1",
   "metadata": {},
   "source": [
    "¿Qué diferencías hubo?  Lo que pasó es que se eliminaron los sufijos de varias palabras para reducir la dimensionalidad de las mismas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8444a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "X = df['news_stemmer']\n",
    "y = df['Type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "#Se mide el rendimiento del modelo\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7b2b86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 198193 stored elements and shape (973, 11930)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vemos dimensionalidad\n",
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a509ee",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "606f4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e5644bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1da5f",
   "metadata": {},
   "source": [
    "NOTA: El siguiente punto al ejecutarlo me tardó 2m con 21.2s por lo que no desesperen si es que se tarda mucho o poco, recordemos que el lemma a pesar de ser de los metodos más sofisticados, este tambien es el que más tiempo se tarda en realizar la operación, conjunto a que son 1216 textos que tiene que lemmatizar por lo que se tomará algo de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abea6486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       durante el foro el banco articulador empresari...\n",
       "1       el regulador de valor de china decir el doming...\n",
       "2       en uno industria históricamente masculino como...\n",
       "3       con el dato de marzo el ipc interanual encaden...\n",
       "4       ayer en cartagena él dar inicio a el versión n...\n",
       "                              ...                        \n",
       "1212    en el vida de todo empresa emergente llegar un...\n",
       "1213    el espiral alcista de el precio continuar y ge...\n",
       "1214    el grande derrota nacional ser experiencia tra...\n",
       "1215    bbva haber alcanzar uno acuerdo de colaboració...\n",
       "1216    casi entrar a el parte final de noviembre el é...\n",
       "Name: new_lemma, Length: 1217, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new_lemma'] = df['news'].apply(lemmatize_text)\n",
    "df['new_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9509b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'con el dato de marzo el ipc interanual encadenar su decimoquinto tasa positivo consecutivo el inflación publicado por el ine él haber mantener igual respecto al avance del de marzo y él situar punto por encima del dato de febrero que ascender al ese punto de diferencia el mayor parte él colocar el grupo de el vivienda punto por el subida de el electricidad y el del transporte punto por el alza de el carburante también impulsar el ipc de marzo el aumento de el precio de el restauración y el servicio de alojamiento y al encarecimiento generalizado de el alimento especialmente del pescado y el marisco de el carne de el legumbr y hortaliza y de el leche el queso y el tener en cuenta el rebaja del impuesto especial sobre el electricidad y el variación sobre otro impuesto el ipc interanual alcanzar en marzo nueve décima más que el tasa general del así él reflejar el ipc a impuesto constante que el ine también publicar en el marco de este inflación subyacente sin alimento no elaborado ni producto energético aumentar en marzo cuatro décima hasta su valor más alto desde septiembre de de este modo el subyacente él situar más de seis punto por debajo de el tasa del ipc el último año el calefacción el alumbrado y el distribución de agua él haber encarecer el aceite y grasa haber elevar su precio uno y el transporte personal ser uno más caro por el mayor coste de el carburante también registrar alza de dos dígito el huevo y el leche uno más caro que hacer uno año y el carne de ovino y el pescado fresco y congelado con repunte del en ambos estar el precio por comunidadescastillala mancho él situar a el cabeza con uno tasa de inflación del seguida de castilla y león aragón el rioja galicia extremadura cantabria y comunidad valenciano el otro el comunidad donde él registrar el menor subida ser canario madrid balear asturia país vasco y cataluña ipc disparar su tasa mensual al tasa mensual el ipc registrar en marzo uno incremento del respecto a febrero su mayor alza mensual en cualquiera mes desde cuando él cambiar el metodología de este estadística para recoger mejor el evolución del mercado echar el vista más atrás tomar serie anterior el repunte mensual de marzo ser el más elevado desde agosto de el tercer mes de el índice de precio de consumo armonizado ipca situar su tasa interanual en más de dos punto por encima de el de febrero por su parte el indicador adelantado del ipca avanzar uno en tasa mensual'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new_lemma'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f516e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7868852459016393\n"
     ]
    }
   ],
   "source": [
    "#Separamos los datos en variables de entrada y etiquetas\n",
    "X = df['new_lemma']\n",
    "y = df['Type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "#Medimos el rendimiento del modelo\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8600edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 191309 stored elements and shape (973, 16423)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vemos dimensionalidad\n",
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f189e5d",
   "metadata": {},
   "source": [
    "## Comparativa de Procesos de Texto\n",
    "\n",
    "| Proceso | Dimensionalidad | Resultado del Modelo |\n",
    "|---------|----------------|---------------------|\n",
    "| Tokenización | 26,760 | 0.8074 |\n",
    "| Stemming | 11,930 | 0.8197 |\n",
    "| Lemmatization | 16,423 | 0.7869 |\n",
    "\n",
    "### Análisis Comparativo\n",
    "\n",
    "- **Mayor dimensionalidad**: Tokenización (26,760 features)\n",
    "- **Mejor resultado del modelo**: Stemming (81.97%)\n",
    "- **Menor dimensionalidad**: Stemming (11,930 features)\n",
    "- **Proceso más balanceado**: Stemming combina buena precisión con reducción dimensional significativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28d7ec",
   "metadata": {},
   "source": [
    "# Introducción a la Similitud de Vectores\n",
    "\n",
    "## ¿Qué es la similitud de vectores?\n",
    "\n",
    "La similitud de vectores, como se puede obviar, es qué tanto separece un vector a otro. Pero ¿Qué tiene que ver esto con NLP?  \n",
    "\n",
    "Como todos sabemos, los modelos de ML no entienden palabras, solo entienden numeros, por lo que lo que se realiza una conversión de cada palabra en un vector, esto para poder medir la distancia entre una palabra y otras para que el modelo pueda comprender mejor las relaciones.\n",
    "\n",
    "Un Ejemplo es el siguiente:\n",
    "- \"mamá\" y \"papá\" estarían cercas (tienen una relación con el concepto de \"familia\").\n",
    "- \"Tomate\" y \"Zanahoria\" estarian cercas (tienen una relación con el concepto de \"Verduras\").\n",
    "- \"Persona\" y \"Arena\" estarían lejos\n",
    "\n",
    "```python\n",
    "# Imagina que cada palabra tiene coordenadas:\n",
    "mom = [0.8, 0.9, 0.7]      # Cerca de \"dad\"\n",
    "dad = [0.7, 0.9, 0.8]    # Cerca de \"mom\"  \n",
    "tomate = [0.1, 0.2, 0.9]  # Cerca de \"zanahoria\"\n",
    "arena = [0.9, 0.1, 0.2] # Lejos de todo lo anterior\n",
    "```\n",
    "\n",
    "## Aplicaciones practicas de la similitud de vectores\n",
    "- Similitud de documentos\n",
    "- Spinning de articulos y SEO\n",
    "- Recomendaciones\n",
    "- Chatbots \n",
    "- Traducción automática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5e13d",
   "metadata": {},
   "source": [
    "# Introducción a Método TF-IDF\n",
    "\n",
    "En NLP, muchas veces, se requiere entender la importancia relativa de ciertas palabras dentro de un o unos documentos.\n",
    "- TF = Term Frequency\n",
    "- IDF = Inverse Document Frequency\n",
    "\n",
    "En resumen: Es un metodo para saber cuan importante es una palabra para un documento/colección.\n",
    "\n",
    "## Problemas con el método del conteo\n",
    "\n",
    "El método del conteo NO toma en cuenta la relevancia de las palabras, cosa que TF-IDF reduce el peso de las palabras comunes y aumenta el peso de las palabras que no se utilizan con frecuencia\n",
    "\n",
    "## Stop Words\n",
    "\n",
    "Recordemos que en los primeros puntos del documento vimos el tema de las stop words donde dichas palabras se eliminaban del texto ya que son palabras comunes (si, la, el, mas, etc). Si dichas palabras se dejan en el texto pueden sesgar el análisis hacia palabras más comunes, es por ello que se excluyen durante el NLP.\n",
    "\n",
    "## Ambigüedad y especifidad de las stop words dependiendo de la aplicación\n",
    "\n",
    "¿Recuerdan la practica de las stop words donde en una frase se nos borraba una palabra que no debia ser clasificada como stop word?\n",
    "\n",
    "Bueno, sabemos que no todas las palabras comunes son stop words en todas las situaciones. Por ejemplo \"no\" podría ser clasificada como stop word en muchos contextos, pero NO en el análisis de sentimientos donde podría ser una palabra clave (Igual tomar el ejemplo que pasó aquí donde se tomó como stop word la palabra \"estás\").\n",
    "\n",
    "## Funcionamiento de TF-IDF\n",
    "\n",
    "TF-IDF asigna un puntaje a cada palabra en un documento en función de su frecencia en ese documento (TF) y su frecuencia en todos los documentos (IDF). Cuanto más a menudo aparece una palabra en un solo documento, pero menos a menudo en todos los documentos, mayor es su puntaje TF-IDF. \n",
    "\n",
    "\"Las palabras que aparecen con más frecuencia en un documento pero raramente en otros documentos son más importantes\".\n",
    "\n",
    "## Formulación de TF-IDF y su justificación\n",
    "\n",
    "La forma en la que se calcula el TF-IDF es multiplicando dos componentes: TF e IDF\n",
    "- TF: Se calcula como el número de veces que aparece una palabra en un documento dividido por el total del palabras en ese documento.\n",
    "- IDF: Se calcula como el logaritmo del total de documentos dividido por el número de documentos que contienen la palabra.\n",
    "\n",
    "### Formulas\n",
    "\n",
    "- TF-IDF = TF(t,d)*IDF(t,D)\n",
    "- TF(t,d) = (Número de veces que el termino \"t\" aparece en el documento \"d\") / (Total de términos en el documento \"d\")\n",
    "- IDF(t,D) = log_e(Total de documentos en el corpus D / Número de documentos donde el término \"t\" aparece)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
